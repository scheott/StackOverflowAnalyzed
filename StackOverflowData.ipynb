{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "051ef1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Id UserId      Name                     Date Class TagBased\n",
      "0  1      4  Informed  2016-08-02T15:38:29.913     3    False\n",
      "1  2      9  Informed  2016-08-02T15:39:20.227     3    False\n",
      "2  3     16  Informed  2016-08-02T15:39:28.290     3    False\n",
      "3  4     18  Informed  2016-08-02T15:39:40.377     3    False\n",
      "4  5     10  Informed  2016-08-02T15:39:56.643     3    False\n"
     ]
    }
   ],
   "source": [
    "import py7zr\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Step 1: Extract the .7z file\n",
    "extract_folder = \"path_to_extract_folder\"\n",
    "with py7zr.SevenZipFile(\"C:\\\\Users\\\\schen\\\\DB_NLP_Project\\\\ai.stackexchange.com.7z\", mode='r') as z:\n",
    "    z.extractall(extract_folder)\n",
    "\n",
    "# Find the extracted file (assuming only one file is in the archive)\n",
    "extracted_files = os.listdir(extract_folder)\n",
    "if not extracted_files:\n",
    "    raise ValueError(\"No files found in the extracted folder!\")\n",
    "data_path = os.path.join(extract_folder, extracted_files[0])\n",
    "\n",
    "# Parsing the XML data\n",
    "tree = ET.parse(data_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Creating an empty list to store rows\n",
    "data = []\n",
    "\n",
    "# Extracting rows from XML\n",
    "for row in root.findall('row'):\n",
    "    data.append(row.attrib)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df1 = pd.DataFrame(data)\n",
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7e3c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444     286\n",
      "8        137\n",
      "1847     101\n",
      "1641      93\n",
      "4302      89\n",
      "1671      83\n",
      "145       71\n",
      "18758     71\n",
      "16909     70\n",
      "16565     69\n",
      "Name: UserId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_users = df['UserId'].value_counts().head(10)\n",
    "print(top_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0459938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e65c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words): # used\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        tempList = []\n",
    "        for word2 in word:\n",
    "            tempList.append(wordlemmatizer.lemmatize(word2))\n",
    "        lemmatized_words.append(tempList)\n",
    "    return lemmatized_words\n",
    "\n",
    "def uniqueWord(w): # used \n",
    "    w2=[]\n",
    "    for word in w:\n",
    "        tempList=[]\n",
    "        for word2 in word:\n",
    "            if tempList.count(word2)<1:\n",
    "                    tempList.append(word2)\n",
    "        w2.append(tempList)\n",
    "    return w2\n",
    "\n",
    "def remove_special_characters(text): # used\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "\n",
    "def removeStopWord(word_text):  # used\n",
    "    filtered_sentence = [] \n",
    "    stop_words = set(stopwords.words('english'))   \n",
    "    for w in word_text:\n",
    "        tempList=[]\n",
    "        for x in w:\n",
    "            if x.lower() not in stop_words: \n",
    "                tempList.append(x)\n",
    "        filtered_sentence.append(tempList)\n",
    "    return filtered_sentence   \n",
    "def meanOfWord(model, sentence): # used\n",
    "#     posValue=nltk.pos_tag(sentence)\n",
    "    posList=['CD']\n",
    "    nounList=['NN','NNP','NNS','NNPS']\n",
    "    value=[]\n",
    "    count=0\n",
    "    noun=0\n",
    "    for word in sentence:\n",
    "        a=model.wv.most_similar(word)\n",
    "        temp=[]\n",
    "        for w in a:\n",
    "            temp.append(w[1])\n",
    "        posValue=nltk.pos_tag([word])\n",
    "#         print(posValue)\n",
    "        wordScore=np.mean(temp)\n",
    "        if posValue[0][1] in posList:\n",
    "            count=count+1\n",
    "        else:\n",
    "            valueIfNum=checkNum(word)\n",
    "            count=count+valueIfNum\n",
    "        if posValue[0][1] in nounList:\n",
    "            noun=noun + .25\n",
    "        value.append(wordScore)\n",
    "    return np.mean(value)+count+noun\n",
    "\n",
    "def checkNum(s):\n",
    "    l= ['1','2','3','4','5','6','7','8','9','0']\n",
    "    check =False\n",
    "\n",
    "    for i in s:\n",
    "        if i in l:\n",
    "            check = True\n",
    "            break\n",
    "    if check == True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "score_list =[] #store date with sentence score and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e92ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m score\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtweets_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(text) \u001b[38;5;66;03m# 1: sent tokenize\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df' is not defined"
     ]
    }
   ],
   "source": [
    "score=[]\n",
    "for i, row in tweets_df.iterrows():\n",
    "    text = row['Text']\n",
    "    sentences = sent_tokenize(text) # 1: sent tokenize\n",
    "    text_noSpecial_character = remove_special_characters(str(text)) # 2: remove special character:\n",
    "    word_text = [[text_noSpecial_character for text_noSpecial_character in sentences.split()] for sentences in sentences] # 3: word token\n",
    "    stop_text= removeStopWord(word_text) # 4: remove stop words\n",
    "    unique_text= uniqueWord(stop_text)   # 5: remove duplicate words\n",
    "    lemma_text = lemmatize_words(unique_text) # 6: lemmatization\n",
    "\n",
    "    model = Word2Vec(lemma_text, min_count=1,sg=1)\n",
    "    \n",
    "    count = 0\n",
    "    for index, sentence in enumerate(lemma_text):\n",
    "        l = lemma_text.index(sentence)\n",
    "        meanScore= meanOfWord(model,sentence)\n",
    "#         print(str(labels[index])+ \":\"+ str(sentence)+ str(meanScore) )\n",
    "        if index == 0:\n",
    "            count = count + meanScore\n",
    "    \n",
    "    if (i  + 1) % 100 == 0:\n",
    "        WeekScore = count / 100\n",
    "        score.append(WeekScore)\n",
    "        count = 0\n",
    "    \n",
    "score_df = pd.DataFrame(score, columns=['SentenceScore'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
